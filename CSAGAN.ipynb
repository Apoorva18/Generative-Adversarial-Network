{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSAGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHysyBijcg65",
        "colab_type": "text"
      },
      "source": [
        "# CSAWGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9F5U5Lhciyh",
        "colab_type": "text"
      },
      "source": [
        "### Spectral Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl4WllCwcPug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine import *\n",
        "from keras.legacy import interfaces\n",
        "from keras import activations\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras.utils.generic_utils import func_dump\n",
        "from keras.utils.generic_utils import func_load\n",
        "from keras.utils.generic_utils import deserialize_keras_object\n",
        "from keras.utils.generic_utils import has_arg\n",
        "from keras.utils import conv_utils\n",
        "from keras.legacy import interfaces\n",
        "from keras.layers import Dense, Conv1D, Conv2D, Conv3D, Conv2DTranspose, Embedding\n",
        "import tensorflow as tf\n",
        "\n",
        "class DenseSN(Dense):\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[-1]\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.units,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                                 initializer=initializers.RandomNormal(0, 1),\n",
        "                                 name='sn',\n",
        "                                 trainable=False)\n",
        "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
        "        self.built = True\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        #Flatten the Tensor\n",
        "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
        "        _u, _v = power_iteration(W_reshaped, self.u)\n",
        "        #Calculate Sigma\n",
        "        sigma=K.dot(_v, W_reshaped)\n",
        "        sigma=K.dot(sigma, K.transpose(_u))\n",
        "        #normalize it\n",
        "        W_bar = W_reshaped / sigma\n",
        "        #reshape weight tensor\n",
        "        if training in {0, False}:\n",
        "            W_bar = K.reshape(W_bar, W_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                 W_bar = K.reshape(W_bar, W_shape)  \n",
        "        output = K.dot(inputs, W_bar)\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output \n",
        "        \n",
        "class _ConvSN(Layer):\n",
        "\n",
        "    def __init__(self, rank,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 strides=1,\n",
        "                 padding='valid',\n",
        "                 data_format=None,\n",
        "                 dilation_rate=1,\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 spectral_normalization=True,\n",
        "                 **kwargs):\n",
        "        super(_ConvSN, self).__init__(**kwargs)\n",
        "        self.rank = rank\n",
        "        self.filters = filters\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n",
        "        self.activation = activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2)\n",
        "        self.spectral_normalization = spectral_normalization\n",
        "        self.u = None\n",
        "        \n",
        "    def _l2normalize(self, v, eps=1e-12):\n",
        "        return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "    \n",
        "    def power_iteration(self, u, W):\n",
        "        '''\n",
        "        Accroding the paper, we only need to do power iteration one time.\n",
        "        '''\n",
        "        v = self._l2normalize(K.dot(u, K.transpose(W)))\n",
        "        u = self._l2normalize(K.dot(v, W))\n",
        "        return u, v\n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "\n",
        "        #Spectral Normalization\n",
        "        if self.spectral_normalization:\n",
        "            self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                                     initializer=initializers.RandomNormal(0, 1),\n",
        "                                     name='sn',\n",
        "                                     trainable=False)\n",
        "        \n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.filters,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_dim})\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        \n",
        "        if self.spectral_normalization:\n",
        "            W_shape = self.kernel.shape.as_list()\n",
        "            #Flatten the Tensor\n",
        "            W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
        "            _u, _v = power_iteration(W_reshaped, self.u)\n",
        "            #Calculate Sigma\n",
        "            sigma=K.dot(_v, W_reshaped)\n",
        "            sigma=K.dot(sigma, K.transpose(_u))\n",
        "            #normalize it\n",
        "            W_bar = W_reshaped / sigma\n",
        "            #reshape weight tensor\n",
        "            if training in {0, False}:\n",
        "                W_bar = K.reshape(W_bar, W_shape)\n",
        "            else:\n",
        "                with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                    W_bar = K.reshape(W_bar, W_shape)\n",
        "\n",
        "            #update weitht\n",
        "            self.kernel = W_bar\n",
        "        \n",
        "        if self.rank == 1:\n",
        "            outputs = K.conv1d(\n",
        "                inputs,\n",
        "                self.kernel,\n",
        "                strides=self.strides[0],\n",
        "                padding=self.padding,\n",
        "                data_format=self.data_format,\n",
        "                dilation_rate=self.dilation_rate[0])\n",
        "        if self.rank == 2:\n",
        "            outputs = K.conv2d(\n",
        "                inputs,\n",
        "                self.kernel,\n",
        "                strides=self.strides,\n",
        "                padding=self.padding,\n",
        "                data_format=self.data_format,\n",
        "                dilation_rate=self.dilation_rate)\n",
        "        if self.rank == 3:\n",
        "            outputs = K.conv3d(\n",
        "                inputs,\n",
        "                self.kernel,\n",
        "                strides=self.strides,\n",
        "                padding=self.padding,\n",
        "                data_format=self.data_format,\n",
        "                dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_last':\n",
        "            space = input_shape[1:-1]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = conv_utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding=self.padding,\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            return (input_shape[0],) + tuple(new_space) + (self.filters,)\n",
        "        if self.data_format == 'channels_first':\n",
        "            space = input_shape[2:]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = conv_utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding=self.padding,\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            return (input_shape[0], self.filters) + tuple(new_space)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'rank': self.rank,\n",
        "            'filters': self.filters,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'strides': self.strides,\n",
        "            'padding': self.padding,\n",
        "            'data_format': self.data_format,\n",
        "            'dilation_rate': self.dilation_rate,\n",
        "            'activation': activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias,\n",
        "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
        "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
        "        }\n",
        "        base_config = super(_Conv, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "    \n",
        "class ConvSN2D(Conv2D):\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.filters,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "            \n",
        "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                         initializer=initializers.RandomNormal(0, 1),\n",
        "                         name='sn',\n",
        "                         trainable=False)\n",
        "        \n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_dim})\n",
        "        self.built = True\n",
        "    def call(self, inputs, training=None):\n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            #Accroding the paper, we only need to do power iteration one time.\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        #Spectral Normalization\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        #Flatten the Tensor\n",
        "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
        "        _u, _v = power_iteration(W_reshaped, self.u)\n",
        "        #Calculate Sigma\n",
        "        sigma=K.dot(_v, W_reshaped)\n",
        "        sigma=K.dot(sigma, K.transpose(_u))\n",
        "        #normalize it\n",
        "        W_bar = W_reshaped / sigma\n",
        "        #reshape weight tensor\n",
        "        if training in {0, False}:\n",
        "            W_bar = K.reshape(W_bar, W_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                W_bar = K.reshape(W_bar, W_shape)\n",
        "                \n",
        "        outputs = K.conv2d(\n",
        "                inputs,\n",
        "                W_bar,\n",
        "                strides=self.strides,\n",
        "                padding=self.padding,\n",
        "                data_format=self.data_format,\n",
        "                dilation_rate=self.dilation_rate)\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "    \n",
        "class ConvSN1D(Conv1D):\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.filters,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "            \n",
        "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                 initializer=initializers.RandomNormal(0, 1),\n",
        "                 name='sn',\n",
        "                 trainable=False)\n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_dim})\n",
        "        self.built = True\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            #Accroding the paper, we only need to do power iteration one time.\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        #Spectral Normalization\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        #Flatten the Tensor\n",
        "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
        "        _u, _v = power_iteration(W_reshaped, self.u)\n",
        "        #Calculate Sigma\n",
        "        sigma=K.dot(_v, W_reshaped)\n",
        "        sigma=K.dot(sigma, K.transpose(_u))\n",
        "        #normalize it\n",
        "        W_bar = W_reshaped / sigma\n",
        "        #reshape weight tensor\n",
        "        if training in {0, False}:\n",
        "            W_bar = K.reshape(W_bar, W_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                W_bar = K.reshape(W_bar, W_shape)\n",
        "                \n",
        "        outputs = K.conv1d(\n",
        "                inputs,\n",
        "                W_bar,\n",
        "                strides=self.strides,\n",
        "                padding=self.padding,\n",
        "                data_format=self.data_format,\n",
        "                dilation_rate=self.dilation_rate)\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "class ConvSN3D(Conv3D):    \n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        \n",
        "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                         initializer=initializers.RandomNormal(0, 1),\n",
        "                         name='sn',\n",
        "                         trainable=False)\n",
        "        \n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.filters,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_dim})\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            #Accroding the paper, we only need to do power iteration one time.\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        #Spectral Normalization\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        #Flatten the Tensor\n",
        "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
        "        _u, _v = power_iteration(W_reshaped, self.u)\n",
        "        #Calculate Sigma\n",
        "        sigma=K.dot(_v, W_reshaped)\n",
        "        sigma=K.dot(sigma, K.transpose(_u))\n",
        "        #normalize it\n",
        "        W_bar = W_reshaped / sigma\n",
        "        #reshape weight tensor\n",
        "        if training in {0, False}:\n",
        "            W_bar = K.reshape(W_bar, W_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                W_bar = K.reshape(W_bar, W_shape)\n",
        "                \n",
        "        outputs = K.conv3d(\n",
        "                inputs,\n",
        "                W_bar,\n",
        "                strides=self.strides,\n",
        "                padding=self.padding,\n",
        "                data_format=self.data_format,\n",
        "                dilation_rate=self.dilation_rate)\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "        \n",
        "class EmbeddingSN(Embedding):\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer=self.embeddings_initializer,\n",
        "            name='embeddings',\n",
        "            regularizer=self.embeddings_regularizer,\n",
        "            constraint=self.embeddings_constraint,\n",
        "            dtype=self.dtype)\n",
        "        \n",
        "        self.u = self.add_weight(shape=tuple([1, self.embeddings.shape.as_list()[-1]]),\n",
        "                         initializer=initializers.RandomNormal(0, 1),\n",
        "                         name='sn',\n",
        "                         trainable=False)\n",
        "        \n",
        "        self.built = True\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        if K.dtype(inputs) != 'int32':\n",
        "            inputs = K.cast(inputs, 'int32')\n",
        "            \n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            #Accroding the paper, we only need to do power iteration one time.\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        W_shape = self.embeddings.shape.as_list()\n",
        "        #Flatten the Tensor\n",
        "        W_reshaped = K.reshape(self.embeddings, [-1, W_shape[-1]])\n",
        "        _u, _v = power_iteration(W_reshaped, self.u)\n",
        "        #Calculate Sigma\n",
        "        sigma=K.dot(_v, W_reshaped)\n",
        "        sigma=K.dot(sigma, K.transpose(_u))\n",
        "        #normalize it\n",
        "        W_bar = W_reshaped / sigma\n",
        "        #reshape weight tensor\n",
        "        if training in {0, False}:\n",
        "            W_bar = K.reshape(W_bar, W_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                W_bar = K.reshape(W_bar, W_shape)\n",
        "        self.embeddings = W_bar\n",
        "            \n",
        "        out = K.gather(self.embeddings, inputs)\n",
        "        return out \n",
        "\n",
        "class ConvSN2DTranspose(Conv2DTranspose):\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if len(input_shape) != 4:\n",
        "            raise ValueError('Inputs should have rank ' +\n",
        "                             str(4) +\n",
        "                             '; Received input shape:', str(input_shape))\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (self.filters, input_dim)\n",
        "\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.filters,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "            \n",
        "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
        "                         initializer=initializers.RandomNormal(0, 1),\n",
        "                         name='sn',\n",
        "                         trainable=False)\n",
        "        \n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n",
        "        self.built = True  \n",
        "    \n",
        "    def call(self, inputs):\n",
        "        input_shape = K.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        if self.data_format == 'channels_first':\n",
        "            h_axis, w_axis = 2, 3\n",
        "        else:\n",
        "            h_axis, w_axis = 1, 2\n",
        "\n",
        "        height, width = input_shape[h_axis], input_shape[w_axis]\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.strides\n",
        "        if self.output_padding is None:\n",
        "            out_pad_h = out_pad_w = None\n",
        "        else:\n",
        "            out_pad_h, out_pad_w = self.output_padding\n",
        "\n",
        "        # Infer the dynamic output shape:\n",
        "        out_height = conv_utils.deconv_length(height,\n",
        "                                              stride_h, kernel_h,\n",
        "                                              self.padding,\n",
        "                                              out_pad_h)\n",
        "        out_width = conv_utils.deconv_length(width,\n",
        "                                             stride_w, kernel_w,\n",
        "                                             self.padding,\n",
        "                                             out_pad_w)\n",
        "        if self.data_format == 'channels_first':\n",
        "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
        "        else:\n",
        "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
        "            \n",
        "        #Spectral Normalization    \n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            #Accroding the paper, we only need to do power iteration one time.\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        W_shape = self.kernel.shape.as_list()\n",
        "        #Flatten the Tensor\n",
        "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
        "        _u, _v = power_iteration(W_reshaped, self.u)\n",
        "        #Calculate Sigma\n",
        "        sigma=K.dot(_v, W_reshaped)\n",
        "        sigma=K.dot(sigma, K.transpose(_u))\n",
        "        #normalize it\n",
        "        W_bar = W_reshaped / sigma\n",
        "        #reshape weight tensor\n",
        "        if training in {0, False}:\n",
        "            W_bar = K.reshape(W_bar, W_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                W_bar = K.reshape(W_bar, W_shape)\n",
        "        self.kernel = W_bar\n",
        "        \n",
        "        outputs = K.conv2d_transpose(\n",
        "            inputs,\n",
        "            self.kernel,\n",
        "            output_shape,\n",
        "            self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCrXoCzBdSEz",
        "colab_type": "text"
      },
      "source": [
        "### Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oN1TdIWdLui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "class Attention(Layer):\n",
        "    def __init__(self, ch, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "        self.channels = ch\n",
        "        self.filters_f_g = self.channels // 8\n",
        "        self.filters_h = self.channels\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
        "        print(kernel_shape_f_g)\n",
        "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
        "\n",
        "        # Create a trainable weight variable for this layer:\n",
        "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n",
        "        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_f')\n",
        "        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_g')\n",
        "        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n",
        "                                        initializer='glorot_uniform',\n",
        "                                        name='kernel_h')\n",
        "        self.bias_f = self.add_weight(shape=(self.filters_f_g,),\n",
        "                                      initializer='zeros',\n",
        "                                      name='bias_F')\n",
        "        self.bias_g = self.add_weight(shape=(self.filters_f_g,),\n",
        "                                      initializer='zeros',\n",
        "                                      name='bias_g')\n",
        "        self.bias_h = self.add_weight(shape=(self.filters_h,),\n",
        "                                      initializer='zeros',\n",
        "                                      name='bias_h')\n",
        "        super(Attention, self).build(input_shape)\n",
        "        \n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "        def hw_flatten(x):\n",
        "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[-1]])\n",
        "\n",
        "        f = K.conv2d(x,\n",
        "                     kernel=self.kernel_f,\n",
        "                     strides=(1, 1), padding='same')  \n",
        "        f = K.bias_add(f, self.bias_f)\n",
        "        g = K.conv2d(x,\n",
        "                     kernel=self.kernel_g,\n",
        "                     strides=(1, 1), padding='same') \n",
        "        g = K.bias_add(g, self.bias_g)\n",
        "        h = K.conv2d(x,\n",
        "                     kernel=self.kernel_h,\n",
        "                     strides=(1, 1), padding='same')  \n",
        "        h = K.bias_add(h, self.bias_h)\n",
        "\n",
        "        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True) \n",
        "\n",
        "        beta = K.softmax(s, axis=-1)  \n",
        "\n",
        "        o = K.batch_dot(beta, hw_flatten(h)) \n",
        "\n",
        "        o = K.reshape(o, shape=K.shape(x))  \n",
        "        x = self.gamma * o + x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7Y4u-Ldl1d",
        "colab_type": "text"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0NiKje2dYMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Concatenate\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.optimizers import RMSprop\n",
        "def generator():\n",
        "  #init = RandomNormal(stddev=0.02)\n",
        "  \n",
        "  #merged = Concatenate()([input, condition])\n",
        "  inp = Input(shape=(50,))\n",
        "  condition= Input(shape =(10,))\n",
        "  merged = Concatenate()([inp, condition])\n",
        "  print(merged.shape)\n",
        " \n",
        "  y = DenseSN(256*4*4,input_shape =(50,))(merged)\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  y = LeakyReLU(alpha =  0.2 )(y)\n",
        "  \n",
        "  \n",
        "  y = Reshape((4,4,256))(y)\n",
        "  \n",
        "  y = ConvSN2DTranspose(128,(4,4),strides = (2,2),padding = 'same')(y)\n",
        "  #model.add(BatchNormalization())\n",
        "  y = LeakyReLU(alpha = 0.2)(y)\n",
        "  #print(y.shape[0])\n",
        "  \n",
        "  \n",
        "\n",
        "  y = ConvSN2DTranspose(128,(4,4),strides = (2,2), padding = 'same')(y)\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  y = LeakyReLU(alpha =0.2)(y)\n",
        "  \n",
        "  y  = Attention(128)(y)\n",
        "  \n",
        "  \n",
        "  \n",
        "  y = ConvSN2DTranspose(128,(4,4),strides = (2,2),padding = 'same')(y)\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  y = LeakyReLU(alpha =0.2)(y)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  \n",
        "  y = ConvSN2DTranspose(3,(3,3),strides = (1,1), padding = 'same')(y)\n",
        "  \n",
        "  y = Activation('tanh')(y)\n",
        "  print(y.shape)\n",
        "  #Adam(lr =0.0001,beta_1=0.0,beta_2 = 0.999)\n",
        "  model = Model(inputs = [inp,condition], output = y)\n",
        "  model.compile(loss=\"binary_crossentropy\",optimizer = Adam(lr =0.0001,beta_1=0.0,beta_2 = 0.999))\n",
        "  \n",
        "  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzoKMFa7ePOs",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4XwPADoeDqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator():\n",
        "  #init = RandomNormal(stddev=0.02)\n",
        "  #const = ClipConstraint(0.01)\n",
        "  \n",
        "  \n",
        "  \n",
        "  input = Input(shape=(32,32,3))\n",
        "  condition = Input(shape= (10,))\n",
        "  y = ConvSN2D(64,(4,4),padding = 'same',input_shape =[32,32,3])(input)\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  y =LeakyReLU(alpha = 0.2)(y)\n",
        " \n",
        "  \n",
        "  y = ConvSN2D(128,(4,4),strides = (2,2),padding = 'same')(y)\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  y = LeakyReLU(alpha = 0.2)(y)\n",
        "  \n",
        "            \n",
        "  y = ConvSN2D(128,(4,4),strides = (2,2),padding = 'same')(y)\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  y = LeakyReLU(alpha = 0.2)(y)\n",
        "  \n",
        "            \n",
        "  y = ConvSN2D(256,(4,4),strides = (2,2),padding = 'same')(y)\n",
        "  #model.add(layers.BatchNormalization())\n",
        "  y = LeakyReLU(alpha = 0.2)(y) \n",
        "  \n",
        "  \n",
        "  y = Flatten()(y)\n",
        "  y = Dropout(0.4)(y)\n",
        "  merged = Concatenate()([y, condition])\n",
        "  \n",
        "  y =Dense(1)(y)\n",
        "  y = Activation('sigmoid')(y)\n",
        "  model = Model(inputs = [input,condition], output = y)\n",
        "  #Adam(lr =0.0004,beta_1=0.0,beta_2= 0.999)\n",
        "  model.compile(loss=\"binary_crossentropy\",optimizer =Adam(lr =0.0004,beta_1=0.0,beta_2= 0.999) )\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkypnmaaeZgm",
        "colab_type": "text"
      },
      "source": [
        "### Data Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l0VIKZteWwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Input, Dense, Conv2D, Add, Dot, Conv2DTranspose, Activation, Reshape, LeakyReLU, Flatten, BatchNormalization\n",
        "#from Spectral import DenseSN, ConvSN2D, ConvSN2DTranspose\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam as adam\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.utils.generic_utils import Progbar\n",
        "from time import time\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "#from selfAttention import selfLayer\n",
        "from skimage.transform import resize\n",
        "fidmin =0\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "import numpy \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_fake(batch_size):\n",
        "  noise = numpy.random.randn(50*batch_size)\n",
        "  noise = noise.reshape(batch_size,50)\n",
        "  x =numpy.random.randint(0,10,batch_size)\n",
        "  \n",
        "  y = to_categorical(x,10)\n",
        "  \n",
        "  outputY = 0\n",
        "  return [noise,y]\n",
        "\n",
        "\n",
        "\n",
        "def create_real_samples(data,labels,batch_size):\n",
        "  \n",
        "  s = numpy.random.randint(0,data.shape[0],batch_size)\n",
        "  t = labels[s]\n",
        "  l = data[s]\n",
        "  y = to_categorical(t,10)\n",
        "  \n",
        "  return [l,y]\n",
        "\n",
        "def get_test(data,labels,batch_size):\n",
        "  \n",
        "  s = numpy.random.randint(0,data.shape[0],batch_size)\n",
        "  \n",
        "  l = data[s]\n",
        "  t = labels[s]\n",
        "  y = to_categorical(t,10)\n",
        "  \n",
        "  return [l,y]\n",
        "\n",
        "\n",
        "train_x= train_x.astype('float32')\n",
        "train_x =(train_x-127.5)/127.5\n",
        "test_x= test_x.astype('float32')\n",
        "test_x =(test_x-127.5)/127.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Istr5_sfN5Q",
        "colab_type": "text"
      },
      "source": [
        "### CSAGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhm1ujSsfJKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "def cSAGAN(g,d):\n",
        "  d.trainable = False\n",
        "  noise,label = g.input\n",
        "  generatedimg = g.output\n",
        "  result = d([generatedimg,label])\n",
        "  model = Model(inputs= [noise,label],outputs = result)\n",
        "  model.compile(loss=wasserstein_loss,optimizer = RMSprop(lr=0.00005))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3LiKrGjfSIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auwPRDimfYnC",
        "colab_type": "text"
      },
      "source": [
        "### FID SCORE CALCULATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLfJ7L_afblP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from numpy import asarray\n",
        "from numpy.random import shuffle\n",
        "from scipy.linalg import sqrtm\n",
        "from skimage.transform import resize\n",
        "def scale_images(images, new_shape):\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn asarray(images_list)\n",
        " \n",
        "# calculate frechet inception distance\n",
        "def calculate_fid(model, images1, images2):\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        "\t# calculate sum squared difference between means\n",
        "\tssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
        "\t# calculate sqrt of product between cov\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\t# check and correct imaginary numbers from sqrt\n",
        "\tif iscomplexobj(covmean):\n",
        "\t\tcovmean = covmean.real\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid\n",
        "\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBbe7uKNftSD",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw2rxtvZfkJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.datasets.mnist import load_data\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "def train(g,d,m,model,batch_size,train_x,test_x):\n",
        "  minfid=1000\n",
        "  img = []\n",
        "  k=0\n",
        "  for i in tqdm(range(2)):\n",
        "    [real_images,real_labels] = create_real_samples(train_x,train_y,batch_size)\n",
        "    real_result = -numpy.ones((batch_size,1))\n",
        "    result_real= d.train_on_batch([real_images,real_labels],real_result)\n",
        "    \n",
        "    \n",
        "    [fake_images,fake_labels] = create_fake(batch_size)\n",
        "    fakei =g.predict([fake_images,fake_labels])\n",
        "    fake_results =numpy.ones((batch_size,1))\n",
        "    result_fake = d.train_on_batch([fakei,fake_labels],fake_results)\n",
        "  \n",
        "  \n",
        "    fake = create_fake(batch_size)\n",
        "    y_fake = -numpy.ones((batch_size,1))\n",
        "    gan_loss= m.train_on_batch(fake,y_fake)\n",
        "    \n",
        "    \n",
        "    tensorboard.on_epoch_end(i+1, {\"D_real_loss\":result_real ,\"D_fake_loss\":result_fake,\"GAN_loss\":gan_loss})\n",
        "    file = open(\"/content/drive/My Drive/Model/gan.txt\",\"a\")\n",
        "    file.write(\"\\n %.2f\"%gan_loss)\n",
        "    file.close()\n",
        "    file = open(\"/content/drive/My Drive/Model/dfake.txt\",\"a\")\n",
        "    file.write(\"\\n %.2f\"%result_fake)\n",
        "    file.close()\n",
        "    file = open(\"/content/drive/My Drive/Model/dreal.txt\",\"a\")\n",
        "    file.write(\"\\n %.2f\"%result_real)\n",
        "    file.close()\n",
        "    \n",
        "    \n",
        "    if((i+1)%500==0):\n",
        "      \n",
        "      fake2 =create_fake(1000)\n",
        "      fakeimage = g.predict(fake2)\n",
        "      image,label = get_test(test_x,test_y,1000)\n",
        "      images1 = scale_images(image, (299,299,3))\n",
        "      images2 = scale_images(fakeimage, (299,299,3))\n",
        "      \n",
        "      fid = calculate_fid(model, images1, images2)\n",
        "      file = open(\"/content/drive/My Drive/Model/FID.txt\",\"a\")\n",
        "      file.write(\"\\n %.2f\"%fid)\n",
        "      file.close()\n",
        "      a.append(fid)\n",
        "      if(fid<minfid):\n",
        "        minfid =fid\n",
        "        fidmin = minfid\n",
        "        fil = \"/content/drive/My Drive/best/gmodel.h5\"\n",
        "        g.save(fil)  \n",
        "        \n",
        "      \n",
        "      \n",
        "      \n",
        "    \n",
        "    if((i+1)%5000==0):\n",
        "      f = \"/content/drive/My Drive/Model/gmodel%d.h5\"%i\n",
        "      g.save(f)  \n",
        "      #print(\"saved\")\n",
        "      f = \"/content/drive/My Drive/Model/dmodel%d.h5\"%i\n",
        "      d.save(f)  \n",
        "      #print(\"saved\")\n",
        "      f = \"/content/drive/My Drive/Model/ganmodel%d.h5\"%i\n",
        "      m.save(f)  \n",
        "      #print(\"saved\")\n",
        "      #save_plot_image(fakeimage,i,11)\n",
        "      \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHIaMMq8g1CA",
        "colab_type": "text"
      },
      "source": [
        "### Load models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM_K6GpLgELI",
        "colab_type": "code",
        "outputId": "5669ad7d-cf30-485a-b749-b725e6d59e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "g =  generator()\n",
        "d = discriminator()\n",
        "m =  cSAGAN(g,d)\n",
        "tensorboard=tf.keras.callbacks.TensorBoard(\n",
        "    log_dir = '/content/drive/My Drive/Model/logs',histogram_freq=0,batch_size=128,write_graph = True, write_grads =True)\n",
        "tensorboard.set_model(m)\n",
        "a=[]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "(?, 60)\n",
            "(1, 1, 128, 16)\n",
            "(?, ?, ?, 3)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"ac...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z24qtozxgIyL",
        "colab_type": "code",
        "outputId": "c3ba4b9d-3fe1-421e-ea5c-0504c32bb32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "train(g,d,m,model,128,train_x, test_x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            " 50%|█████     | 1/2 [00:15<00:15, 15.57s/it]/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "100%|██████████| 2/2 [00:22<00:00, 13.07s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8VpmDe4gv-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "g = tf.keras.models.load_model('/content/gmodel.h5')\n",
        "import numpy\n",
        "fake2 = numpy.random.normal(0,1,[64,50])\n",
        "fakeimage = g.predict(fake2)\n",
        "from matplotlib import pyplot as plt\n",
        "fakeimage = (fakeimage + 1)/2.0\n",
        "for i in range(8*8):\n",
        "  plt.subplot(8,8,1+i)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(fakeimage[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUrlc2ofiQyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}